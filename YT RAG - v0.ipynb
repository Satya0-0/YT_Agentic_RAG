{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164b74c-0cc0-4193-a866-21e894293746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------All required installations--------\n",
    "\n",
    "# ---- To dowload YouTube Videos ----\n",
    "!pip install -qU \"yt-dlp[default]\"\n",
    "\n",
    "# ---- To use Whisper ----\n",
    "!pip install transformers\n",
    "\n",
    "# ---- LangChain ----\n",
    "!pip install -qU langchain langchain_experimental\n",
    "\n",
    "# ---- To use Google's Gemini Models ----\n",
    "!pip install -qU langchain-google-genai\n",
    "\n",
    "# ---- Vector Database: Chroma ----\n",
    "!pip -qU chromadb langchain_chroma\n",
    "\n",
    "# ---- Updateing `Protocol Buffer` module due to compactibility issues with Gemini Models ----\n",
    "!pip install --upgrade --force-reinstall protobuf\n",
    "\n",
    "# ---- Installing 'typing' for Type Hints ----\n",
    "!pip install -qU typing\n",
    "\n",
    "# ---- Intalling Pydantic for Data Validation (Used in WebSearch) ----\n",
    "!pip install -qU pydantic\n",
    "\n",
    "# # ---- Serper's API Wrapper's dependencies ----\n",
    "# !pip install -qU google-search-results\n",
    "\n",
    "# ---- DuckDuckgo Search ----\n",
    "!pip install -qU duckduckgo-search langchain-community\n",
    "!pip install -U ddgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d4bd6-e874-4de7-9775-7196f9b783df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required librarires\n",
    "\n",
    "\n",
    "# ---- General ----\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ---- To download YouTube Videos - \"yt-dlp\" ----\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "\n",
    "# ---- To use Whisher ----\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# ---- Used for Chucking the documents and create Vector Enbeddings to be used for storage in VectorDB ----\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ---- Using ChromaDB and creating Documents ----\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ---- Document Retriever ----\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from typing import List, Any\n",
    "\n",
    "# ---- Output Parases and LangChain's Runnable Classes ----\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# ---- Using Google's GenAI Models ----\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ---- Prompt Templates of LangChain ----\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ---- For HTTP requests -> To be used in API calls ----\n",
    "import requests\n",
    "\n",
    "# # ---- Serper API ----\n",
    "# from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "# ---- DuckDuckGo ----\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# ---- Tools ----\n",
    "from langchain_core.tools import tool\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "# ---- LangChain LLM Model initializations and LangGraph's Agent's Fundamental Classes  ----\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# ---- Using Pydant's BaseModel for Data Validation ----\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ---- Importing requried Typing Hints -> To be used in creating State ----\n",
    "from typing import TypedDict, List, Annotated, Optional, NotRequired\n",
    "\n",
    "# ---- To be used in Maintaining Message History ----\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# ---- For Persistence ----\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ---- For Graph ----\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ---- for RegEx (used in YouTube Video Title Handling) ----\n",
    "import re\n",
    "\n",
    "# ---- To be used in generating Unique FileName ----\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5acfdf7-7322-4795-88d5-6b80dc141b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the required API Keys\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821e957-bc42-4753-8c31-e967095e0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "MODEL_INSTANCE = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, timeout=10, max_retries=2)\n",
    "\n",
    "OUTPUT_PARSER = StrOutputParser()\n",
    "\n",
    "SENTENCE_TRANSFORMER = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "TRANSCRIBER = pipeline(\n",
    "    model=\"openai/whisper-small\", \n",
    "    framework = \"pt\", \n",
    "    device = -1, \n",
    "    return_timestamps=True\n",
    ")\n",
    "\n",
    "# Added a Global variable for Vector DB\n",
    "\n",
    "GLOBAL_VECTOR_STORE = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa55676-c43d-4ca5-9492-79cfc545c068",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FFD700;\">Agentic RAG</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e2ebd-7ebc-4278-9bd5-27f7a4dc9716",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #AAF1D1;\">Defining the Nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cea86a-d729-4345-9658-cef199847794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the State:\n",
    "\n",
    "class State(BaseModel):\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        \n",
    "    user_query: str = None\n",
    "    youtubeURL: str = None\n",
    "    localpath: str = r\"\"       #Enter local your path here -> for YouTube Video Downloads\n",
    "    video_details: str = \"\"\n",
    "    transcription: str = \"\"\n",
    "    vectorDB_flg: bool = False\n",
    "    rewritten_query: Optional[str] = None\n",
    "    rewritten_flg : bool = False\n",
    "    retrieval_sync : bool = True\n",
    "    documents: List[str] = []\n",
    "    webResults: Optional[str] = None\n",
    "    graph_output: str = \"\"\n",
    "    graph_exit: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b196f0-ffb4-4ff8-8e1c-2636ce7d019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-1: Downloading YouTube Video\n",
    "\n",
    "\n",
    "# Define the utility function *outside* the node for cleaner code\n",
    "def clean_filename(title: str) -> str:\n",
    "    forbidden_chars = r'[<>:\"/\\\\|?*]'\n",
    "    cleaned_title = re.sub(forbidden_chars, '_', title)\n",
    "    cleaned_title2 = re.sub(' ', '_', cleaned_title)\n",
    "    return re.sub(r'__+', '_', cleaned_title2).strip('_')\n",
    "\n",
    "\n",
    "def Node1_YTVideoDownload(state: State) -> dict:\n",
    "    \n",
    "    # Generating a unique, and safe temporary filename using 'uuid'\n",
    "    temp_filename_base = str(uuid.uuid4())\n",
    "    \n",
    "    # Extracting Video Info (Title and Extension)\n",
    "    with YoutubeDL({}) as yt:\n",
    "        info = yt.extract_info(state.youtubeURL, download=False)\n",
    "        raw_title = info.get(\"title\")\n",
    "        extension = info.get(\"ext\")\n",
    "\n",
    "    # Cleaning the Video Title name (to be used for downloading)\n",
    "    safe_title_base = clean_filename(raw_title)\n",
    "    target_filename = safe_title_base + \".\" + extension\n",
    "    \n",
    "    # Using \"Temporary\" name to download the file\n",
    "    temp_download_name = temp_filename_base + \".\" + extension\n",
    "    temp_path = os.path.join(state.localpath, temp_download_name)\n",
    "    \n",
    "    # Actual download (using Temporar Name and Path)\n",
    "    yt_opts = {\n",
    "        'format': 'bestaudio',\n",
    "        'outtmpl': temp_path\n",
    "    }\n",
    "    \n",
    "    #Changing path for `os's current directory` -> this is where the download takes place Locally\n",
    "    os.chdir(state.localpath) \n",
    "\n",
    "    with YoutubeDL(yt_opts) as yt:\n",
    "        yt.download([state.youtubeURL]) \n",
    "    \n",
    "    # Renaming the downloaded file name from \"Temporary Name\" to \"Clean Name\"\n",
    "\n",
    "    final_path = os.path.join(state.localpath, target_filename)\n",
    "    try:\n",
    "        os.rename(temp_path, final_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error renaming file from {temp_path} to {final_path}: {e}\")\n",
    "        raise \n",
    "\n",
    "    print(\"Node-1 Executed!\")\n",
    "    return {\"video_details\": target_filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d1a66-ad4d-4550-8d08-2f6c7fc28615",
   "metadata": {},
   "source": [
    "* Actual Node-1 here (Double click to expand)\n",
    "<!-- \n",
    "\n",
    "def Node1_YTVideoDownload(state: State) -> dict:\n",
    "    yt_opts = {\n",
    "        'format': 'bestaudio',      # Downloading the best audio format -> High res audio is useful for better Transcriting\n",
    "        'outtmpl': '%(title)s.%(ext)s'  # Saving the file with its title and original extension\n",
    "    }\n",
    "\n",
    "    os.chdir(state.localpath)\n",
    "    \n",
    "    with YoutubeDL(yt_opts) as yt:\n",
    "        yt.download(state.youtubeURL)                                        # Downloading the YouTube Video\n",
    "        info = yt.extract_info(state.youtubeURL, download=False)             # Using \"extract_info\" attribute to get the video details from the URL\n",
    "        title = info.get(\"title\")\n",
    "        extension = info.get(\"ext\")\n",
    "        video_title = (title + \".\" + extension)\n",
    "\n",
    "\n",
    "    import re \n",
    "    \n",
    "    def clean_filename(title: str) -> str:\n",
    "        \"\"\"Removes illegal characters from a string to make it a valid filename.\"\"\"\n",
    "        # List of characters forbidden in Windows filenames\n",
    "        forbidden_chars = r'[<>:\"/\\\\|?*]'\n",
    "        \n",
    "        # Replace forbidden characters with an underscore or a space\n",
    "        cleaned_title = re.sub(forbidden_chars, '_', title)\n",
    "        cleaned_title2 = re.sub(' ', '_', cleaned_title)\n",
    "        \n",
    "        # Limiting the length and clean up multiple spaces/underscores\n",
    "        cleaned_title = re.sub(r'__+', '_', cleaned_title2).strip('_')\n",
    "        \n",
    "        return cleaned_title\n",
    "\n",
    "    cleaned_video_title = clean_filename(video_title)\n",
    "    \n",
    "    return {\"video_details\" : cleaned_video_title} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285086a-649e-4695-937c-66a377ca7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-2: Video Transcription using a Local Model (OpenAI's Whisper)\n",
    "\n",
    "def Node2_Transcription(state: State) -> dict:\n",
    "    \n",
    "    # Defining the local path for the video\n",
    "    audio_path = os.path.join(state.localpath,state.video_details)\n",
    "\n",
    "    text_out = TRANSCRIBER(audio_path)\n",
    "    \n",
    "    # Final Text to be stored in Vector DB\n",
    "    transcribed_text = text_out['text']\n",
    "    print(\"Node-2 Executed!\")\n",
    "\n",
    "    return {\"transcription\" : transcribed_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979305a-e73b-42fe-8283-ba7eba33fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-3: Creating a VectorDB Collection\n",
    "\n",
    "def Node3_vectorDB(state: State) -> dict:\n",
    "    global GLOBAL_VECTOR_STORE\n",
    "    \n",
    "    # Splitting the transcribed text\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    docs = text_splitter.create_documents([state.transcription])\n",
    "    \n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"YT_RAG\", \n",
    "        embedding_function= SENTENCE_TRANSFORMER\n",
    "    )\n",
    "    \n",
    "    # Adding documents to ChromaDB Collection - \"YT_RAG\"\n",
    "    \n",
    "    document_ids = list(f\"id_{x}\" for x in range(len(docs)))\n",
    "    \n",
    "    vector_store.add_documents(documents=docs, ids = document_ids)\n",
    "\n",
    "    GLOBAL_VECTOR_STORE = vector_store\n",
    "    \n",
    "    print(\"Node-3 Executed!\")\n",
    "    \n",
    "    return {\"vectorDB_flg\": True}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f62d86-5158-4ad0-a4fc-ed167b751964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-4: Retriever\n",
    "def Node4_Retriever(state: State) -> dict:\n",
    "    global GLOBAL_VECTOR_STORE\n",
    "    \n",
    "    # Access from global variable\n",
    "    vector_store = GLOBAL_VECTOR_STORE\n",
    "    \n",
    "    class MyRetriever(BaseRetriever):\n",
    "        class Config:\n",
    "            arbitrary_types_allowed = True\n",
    "    \n",
    "        vdb: Chroma\n",
    "        top_k: int = 3\n",
    "    \n",
    "        def _get_relevant_documents(self, qry, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "            results = self.vdb.similarity_search(query=qry, k=self.top_k)\n",
    "            return results\n",
    "\n",
    "        def get_relevant_documents(self, query):\n",
    "            return self._get_relevant_documents(query, CallbackManagerForRetrieverRun)\n",
    "    \n",
    "    retriever = MyRetriever(vdb=vector_store, top_k=3)\n",
    "    query = state.rewritten_query if state.rewritten_flg else state.user_query\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    docs_list = list(doc.page_content for doc in docs)\n",
    "\n",
    "    print(\"Node-4 Executed!\")\n",
    "    return {\"documents\": docs_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8500c-dde6-4964-8d9f-de14b7c605fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-5: LLM Judge (determines the relevance of retrieved documents)\n",
    "\n",
    "def Node5_llmJudge(state: State) -> dict:\n",
    "    LLM_Judge = MODEL_INSTANCE\n",
    "    \n",
    "    Judge_sysMessage = \"\"\"You are acting as a Judge for determining the relevance of provided input documents in the context\n",
    "    and comparing it with the query.\n",
    "    If you think the provided prompts contain the relevant details required to answer the question, say \"True\", otherwise, say \"False\".\n",
    "    You will not generate any additional response other than a single word - \"True\" or \"False\".\n",
    "    Context is : {context}\n",
    "    \\n\n",
    "    Query is: {user_input}\"\"\"\n",
    "\n",
    "    judge_prompt = ChatPromptTemplate([ \n",
    "            (\"system\", Judge_sysMessage), \n",
    "            (\"human\", \"Context: {context} \\n Query: {user_input}\")\n",
    "        ])\n",
    "        \n",
    "\n",
    "    judge_chain = (\n",
    "        judge_prompt\n",
    "        | LLM_Judge\n",
    "        | OUTPUT_PARSER\n",
    "    )\n",
    "\n",
    "    if state.rewritten_flg:\n",
    "        query_to_use = state.rewritten_query\n",
    "    else:\n",
    "        query_to_use = state.user_query\n",
    "\n",
    "    judgement = judge_chain.invoke({\"context\": state.documents, \"user_input\": query_to_use})\n",
    "\n",
    "    judgement_result = judgement.lower() == \"true\"\n",
    "\n",
    "    print(\"Node-5 Executed!\")\n",
    "    \n",
    "    return {\"retrieval_sync\": judgement_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da0377-f7a9-4ac2-89dd-0e77a95d4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-6C: Conditional Node - Rewriting the query (depends on the State variable \"retrieval_sync\")\n",
    "\n",
    "\n",
    "def Node6C_QueryRewriter(state: State) -> dict:\n",
    "\n",
    "    LLM_Query_Rewrite = MODEL_INSTANCE\n",
    "    \n",
    "    rewrite_sysMessage = \"\"\"You are an expert **Query Rewriter** for a sophisticated **Agentic RAG (Retrieval-Augmented Generation) system**. Your sole task is to take a user's initial query and rewrite it into a **highly optimized, self-contained, and comprehensive search query** that will maximize the chances of retrieving relevant documents from a technical knowledge base.\n",
    "    \n",
    "    **Key principles for the rewritten query:**\n",
    "    1.  **Contextualization:** Expand acronyms, define ambiguous terms, or add missing context implied by the original conversation (if applicable, though for a first-pass query, focus on being self-contained).\n",
    "    2.  **Explicitness:** Convert vague questions (e.g., \"how to fix that?\") into concrete statements or specific requests (e.g., \"what are the troubleshooting steps for error code 503 on the Kubernetes control plane?\").\n",
    "    3.  **Keyword Density:** Increase the number of relevant, specific technical terms that an indexing system would recognize.\n",
    "    4.  **Stand-alone Clarity:** The rewritten query **must** make sense and be an effective search term even if the original query is lost.\n",
    "    \n",
    "    Original User Query: {user_input}\"\"\"\n",
    "    \n",
    "    rewrite_prompt = ChatPromptTemplate([\n",
    "        (\"system\", rewrite_sysMessage),\n",
    "        (\"human\", \"{user_input}\")\n",
    "    ])\n",
    "\n",
    "    rewrite_chain = (rewrite_prompt | LLM_Query_Rewrite | OUTPUT_PARSER)\n",
    "\n",
    "    rewritten_query = rewrite_chain.invoke({\"user_input\": state.user_query})\n",
    "\n",
    "    print(\"Node-6 Executed!\")\n",
    "    \n",
    "    return{\"rewritten_query\": rewritten_query, \"rewritten_flg\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3a58e-6048-415c-a218-73109cd54f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-7C: Conditional Node - Performs a Websearch if the re-written query also doesn't fetch the required docs\n",
    "\n",
    "def Node7C_WebSearch(state: State) -> dict:\n",
    "    \"\"\"Used to perform a web search using DuckDuckGo for the given (rewritten) query\"\"\"\n",
    "\n",
    "    if not state.rewritten_query:\n",
    "        # to prevent a failed tool call.\n",
    "        print(\"ERROR: WebSearch Node reached without a rewritten_query. Skipping web search.\")\n",
    "        return {\"webResults\": \"Web search skipped due to missing rewritten query.\"}\n",
    "    \n",
    "    search = DuckDuckGoSearchRun()\n",
    "    webResult = search.invoke(state.rewritten_query)\n",
    "\n",
    "    print(\"Node-7 Executed!\")\n",
    "    print(f\"Search result: {webResult}\")  # for Debugging\n",
    "\n",
    "    return {\"webResults\" :webResult}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b95cec5-d472-4fb6-ad71-3eb53a29b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-8: Response Generator Node\n",
    "\n",
    "def Node8_GenerateResponse(state: State) -> dict:\n",
    "    Generate_LLM = MODEL_INSTANCE\n",
    "\n",
    "    Response_sysMessage = \"\"\"\n",
    "    You are a smart assistant which takes in the given query and generates a response using the provided context data.\n",
    "    If the provided data is insuffucient to answer the question, reply with I don't know.\n",
    "    The provided context data can be from retrieved documents or from web. Always mention the source of getting this data.\n",
    "    Context is : {context}\n",
    "    \\n\n",
    "    Query is: {user_input}\"\"\"\n",
    "    \n",
    "    generation_prompt = ChatPromptTemplate(\n",
    "        [\n",
    "            (\"system\", Response_sysMessage),\n",
    "            (\"human\", \"{user_input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response_chain = (generation_prompt | Generate_LLM | OUTPUT_PARSER)\n",
    "\n",
    "    query = state.rewritten_query if state.rewritten_flg else  state.user_query\n",
    "\n",
    "    context = state.documents if state.retrieval_sync else state.webResults\n",
    "    \n",
    "    result = response_chain.invoke({\"context\": context, \"user_input\": query})\n",
    "\n",
    "    print(\"Node-8 Executed!\")\n",
    "    print(result)\n",
    "    \n",
    "    return {\"graph_output\" : result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ba9ba-0d3f-48d9-93eb-5200cd090ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-9: Placeholder Query. Used for \"interrupt()\"\n",
    "\n",
    "def Node9_getUserInput(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    Used for taking in user input\n",
    "    \"\"\"\n",
    "     # Taking in the New User Input, incase if the graph reached \"get_next_query\", and is waiting for new input\n",
    "    next_query = input(\"\\n>>What's your next question (or type 'quit' to exit): \")\n",
    "    \n",
    "    # Check if the graph reached END during the last run\n",
    "    \n",
    "    exitDecision = True if next_query.lower() == \"quit\" else False\n",
    "        \n",
    "    print(\"Node-9 Executed!\")\n",
    "    \n",
    "    return {    \"graph_exit\": exitDecision,\n",
    "                \"user_query\": next_query,\n",
    "                \"rewritten_query\": None,  # Reset for new query\n",
    "                \"rewritten_flg\": False,   # Reset flag\n",
    "                \"documents\": [],          # Clear old docs\n",
    "                \"webResults\": None        # Clear old web results\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe42b4-d705-4003-b5ba-39b3482c3d87",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #AAF1D1;\">Building the Graph</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfdc7db-fd4a-4692-9e07-783d74d09b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Graph\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Adding nodes\n",
    "\n",
    "graph.add_node(\"1_YTVideoDownload\", Node1_YTVideoDownload)\n",
    "graph.add_node(\"2_Transcription\", Node2_Transcription)\n",
    "graph.add_node(\"3_vectorDB\", Node3_vectorDB)\n",
    "graph.add_node(\"4_Retriever\", Node4_Retriever)\n",
    "graph.add_node(\"5_llmJudge\", Node5_llmJudge)\n",
    "graph.add_node(\"6_QueryRewriter\", Node6C_QueryRewriter)\n",
    "graph.add_node(\"7_WebSearch\", Node7C_WebSearch)\n",
    "graph.add_node(\"8_GenerateResponse\", Node8_GenerateResponse)\n",
    "graph.add_node(\"9_getUserInput\", Node9_getUserInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df992300-99f2-4a18-8197-567ebdacd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Routing Functions\n",
    "\n",
    "# To check if the VectorDB is laready created for the video\n",
    "def vectorDBExists(state: State) -> bool:\n",
    "    global GLOBAL_VECTOR_STORE\n",
    "    \n",
    "    if state.vectorDB_flg and GLOBAL_VECTOR_STORE is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# To check if the initial documents retired using \"Actual User Query\" are relevant or not and then determine the next course of action\n",
    "\n",
    "def retrievedDocsRelevant(state: State) -> str:\n",
    "    if state.retrieval_sync:\n",
    "        nextNode = \"Response\"\n",
    "    \n",
    "    elif not state.retrieval_sync and state.rewritten_flg:\n",
    "        nextNode = \"webSearch\"\n",
    "        \n",
    "    elif not state.retrieval_sync and not state.rewritten_flg:\n",
    "        nextNode = \"Rewrite\"\n",
    "    \n",
    "    else:\n",
    "        # Fallback (shouldn't reach here)\n",
    "        return \"Response\"\n",
    "    \n",
    "    return nextNode\n",
    "\n",
    "def graphExit(state: State) -> bool:\n",
    "    return state.graph_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07cbaa9-514e-4c28-8713-f5ea07748ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Edges to the \"graph\" instance\n",
    "\n",
    "graph.add_conditional_edges(START, vectorDBExists, {True: \"4_Retriever\", False: \"1_YTVideoDownload\"})\n",
    "graph.add_edge(\"1_YTVideoDownload\", \"2_Transcription\")\n",
    "graph.add_edge(\"2_Transcription\", \"3_vectorDB\")\n",
    "graph.add_edge(\"3_vectorDB\", \"4_Retriever\")\n",
    "graph.add_edge(\"4_Retriever\", \"5_llmJudge\")\n",
    "graph.add_conditional_edges(\"5_llmJudge\", retrievedDocsRelevant, {\"Response\": \"8_GenerateResponse\", \"Rewrite\": \"6_QueryRewriter\", \"webSearch\": \"7_WebSearch\"})\n",
    "graph.add_edge(\"6_QueryRewriter\", \"4_Retriever\")\n",
    "graph.add_edge(\"7_WebSearch\", \"8_GenerateResponse\")\n",
    "graph.add_edge(\"8_GenerateResponse\", \"9_getUserInput\")\n",
    "graph.add_conditional_edges(\"9_getUserInput\", graphExit, {True: END, False: \"4_Retriever\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c260fe-a7ab-4d71-88df-a7005af3b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compling the Graph\n",
    "# Using a memory saver so the state (like vectorDB) persists between runs\n",
    "\n",
    "app = graph.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# A unique ID for this conversation\n",
    "thread_id = \"chat-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110d8fb-079b-4461-a305-b011baa8230b",
   "metadata": {},
   "source": [
    "<h1 style=\"color: orange\">Input Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07ec57-0733-4237-9ad1-96231956228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking YT Video as an input for Q&A:\n",
    "\n",
    "yt_url = input(\">> Enter the YouTube URL for Q&A:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7ceec-dd7f-437d-b9db-51b02f50d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking in Initial User Query\n",
    "\n",
    "initial_user_query = input(\">>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35450b5-8ea8-4fb7-b91e-31f508ebfd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ACTUAL CONVERSATION ---\n",
    "\n",
    "# Starting the Graph with Initial User Query\n",
    "app.update_state(\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id\n",
    "        }\n",
    "    },\n",
    "    values={\n",
    "        \"user_query\": initial_user_query,\n",
    "        \"youtubeURL\": yt_url\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a79724-446b-4423-81c4-869965f31e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke({}, config={\"configurable\": {\"thread_id\": thread_id}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233f67b-fe54-48e2-a72b-398702e207fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "list(app.get_state_history(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca052d1-0e5e-4562-9ad1-71431eadf56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
